{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd2b2eba-b7fd-4856-960f-f2cbadcc12af",
   "metadata": {},
   "source": [
    "# Building a Exa Search Powered Data Agent\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/run-llama/llama-hub/blob/main/llama_hub/tools/notebooks/exa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "This tutorial walks through using the LLM tools provided by the [Exa API](https://exa.ai) to allow LLMs to use semantic queries to search for and retrieve rich web content from the internet.\n",
    "\n",
    "To get started, you will need an [OpenAI api key](https://platform.openai.com/account/api-keys) and an [Exa API key](https://dashboard.exa.ai/api-keys)\n",
    "\n",
    "We will import the relevant agents and tools and pass them our keys here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa61e0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index\n",
      "  Using cached llama_index-0.10.54-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llama-index-core\n",
      "  Using cached llama_index_core-0.10.53.post1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-tools-exa\n",
      "  Using cached llama_index_tools_exa-0.1.3-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index)\n",
      "  Using cached llama_index_agent_openai-0.2.8-py3-none-any.whl.metadata (729 bytes)\n",
      "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Using cached llama_index_cli-0.1.12-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index)\n",
      "  Using cached llama_index_embeddings_openai-0.1.10-py3-none-any.whl.metadata (604 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama-index)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.2.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
      "  Using cached llama_index_legacy-0.9.48-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama-index)\n",
      "  Using cached llama_index_llms_openai-0.1.25-py3-none-any.whl.metadata (610 bytes)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index)\n",
      "  Using cached llama_index_multi_modal_llms_openai-0.1.7-py3-none-any.whl.metadata (728 bytes)\n",
      "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index)\n",
      "  Using cached llama_index_program_openai-0.1.6-py3-none-any.whl.metadata (715 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Using cached llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index)\n",
      "  Using cached llama_index_readers_file-0.1.29-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.1.2 (from llama-index)\n",
      "  Using cached llama_index_readers_llama_parse-0.1.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from llama-index-core) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from llama-index-core) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from llama-index-core) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from llama-index-core) (1.2.14)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core)\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from llama-index-core) (2024.6.0)\n",
      "Requirement already satisfied: httpx in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from llama-index-core) (0.27.0)\n",
      "Collecting llama-cloud<0.0.7,>=0.0.6 (from llama-index-core)\n",
      "  Using cached llama_cloud-0.0.6-py3-none-any.whl.metadata (750 bytes)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from llama-index-core) (1.6.0)\n",
      "Collecting networkx>=3.0 (from llama-index-core)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting nltk<4.0.0,>=3.8.1 (from llama-index-core)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy<2.0.0 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from llama-index-core) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from llama-index-core) (1.34.0)\n",
      "Requirement already satisfied: pandas in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from llama-index-core) (2.2.2)\n",
      "Collecting pillow>=9.0.0 (from llama-index-core)\n",
      "  Using cached pillow-10.4.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from llama-index-core) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from llama-index-core) (8.3.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from llama-index-core) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from llama-index-core) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from llama-index-core) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from llama-index-core) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from llama-index-core) (1.16.0)\n",
      "Requirement already satisfied: exa-py<2.0.0,>=1.0.8 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from llama-index-tools-exa) (1.0.12)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.9.4)\n",
      "Requirement already satisfied: pydantic>=1.10 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from llama-cloud<0.0.7,>=0.0.6->llama-index-core) (2.7.4)\n",
      "Requirement already satisfied: anyio in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from httpx->llama-index-core) (4.4.0)\n",
      "Requirement already satisfied: certifi in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from httpx->llama-index-core) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from httpx->llama-index-core) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from httpx->llama-index-core) (3.7)\n",
      "Requirement already satisfied: sniffio in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from httpx->llama-index-core) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core) (0.14.0)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.2.0)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
      "  Using cached striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index)\n",
      "  Using cached llama_parse-0.4.6-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: click in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core) (8.1.7)\n",
      "Collecting joblib (from nltk<4.0.0,>=3.8.1->llama-index-core)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core) (2023.12.25)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-core) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core) (2.2.1)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core)\n",
      "  Using cached greenlet-3.0.3-cp311-cp311-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from dataclasses-json->llama-index-core) (3.21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from pandas->llama-index-core) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from pandas->llama-index-core) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from pandas->llama-index-core) (2024.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core) (2.18.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/vishalkhanna/Documents/projects/exa-crag-example/.conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core) (1.16.0)\n",
      "Using cached llama_index-0.10.54-py3-none-any.whl (6.8 kB)\n",
      "Using cached llama_index_core-0.10.53.post1-py3-none-any.whl (15.4 MB)\n",
      "Using cached llama_index_tools_exa-0.1.3-py3-none-any.whl (3.6 kB)\n",
      "Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Using cached llama_cloud-0.0.6-py3-none-any.whl (130 kB)\n",
      "Using cached llama_index_agent_openai-0.2.8-py3-none-any.whl (13 kB)\n",
      "Using cached llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\n",
      "Using cached llama_index_embeddings_openai-0.1.10-py3-none-any.whl (6.2 kB)\n",
      "Using cached llama_index_indices_managed_llama_cloud-0.2.4-py3-none-any.whl (9.2 kB)\n",
      "Using cached llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
      "Using cached llama_index_llms_openai-0.1.25-py3-none-any.whl (11 kB)\n",
      "Using cached llama_index_multi_modal_llms_openai-0.1.7-py3-none-any.whl (5.9 kB)\n",
      "Using cached llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
      "Using cached llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
      "Using cached llama_index_readers_file-0.1.29-py3-none-any.whl (38 kB)\n",
      "Using cached llama_index_readers_llama_parse-0.1.6-py3-none-any.whl (2.5 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached pillow-10.4.0-cp311-cp311-macosx_11_0_arm64.whl (3.4 MB)\n",
      "Using cached greenlet-3.0.3-cp311-cp311-macosx_11_0_universal2.whl (271 kB)\n",
      "Using cached llama_parse-0.4.6-py3-none-any.whl (9.1 kB)\n",
      "Using cached striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: striprtf, dirtyjson, pillow, networkx, joblib, greenlet, nltk, llama-cloud, llama-index-legacy, llama-index-core, llama-parse, llama-index-tools-exa, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
      "Successfully installed dirtyjson-1.0.8 greenlet-3.0.3 joblib-1.4.2 llama-cloud-0.0.6 llama-index-0.10.54 llama-index-agent-openai-0.2.8 llama-index-cli-0.1.12 llama-index-core-0.10.53.post1 llama-index-embeddings-openai-0.1.10 llama-index-indices-managed-llama-cloud-0.2.4 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.25 llama-index-multi-modal-llms-openai-0.1.7 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.29 llama-index-readers-llama-parse-0.1.6 llama-index-tools-exa-0.1.3 llama-parse-0.4.6 networkx-3.3 nltk-3.8.1 pillow-10.4.0 striprtf-0.0.26\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the relevant LlamaIndex packages, incl. core and Exa tool\n",
    "!pip install llama-index llama-index-core llama-index-tools-exa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df2a0ecd-22e9-4cef-b069-89e4286e4d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search\n",
      "retrieve_documents\n",
      "search_and_retrieve_documents\n",
      "search_and_retrieve_highlights\n",
      "find_similar\n",
      "current_date\n"
     ]
    }
   ],
   "source": [
    "# Get OS for environment variables\n",
    "import os\n",
    "# Set up OpenAI\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "\n",
    "# NOTE:\n",
    "# You must have an OpenAI API key in the environment variable OPENAI_API_KEY\n",
    "# You must have an Exa API key in the environment variable EXA_API_KEY\n",
    "\n",
    "# Set up the Exa search tool\n",
    "from llama_index.tools.exa import ExaToolSpec\n",
    "\n",
    "#Instantiate\n",
    "exa_tool = ExaToolSpec(\n",
    "    api_key=os.environ[\"EXA_API_KEY\"],\n",
    "    # max_characters=2000   # this is the default\n",
    ")\n",
    "\n",
    "# Get the list of tools to see what Exa offers\n",
    "exa_tool_list = exa_tool.to_tool_list()\n",
    "for tool in exa_tool_list:\n",
    "    print(tool.metadata.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e3012-bab0-4e55-858a-e3721282552c",
   "metadata": {},
   "source": [
    "## Testing the Exa tools\n",
    "\n",
    "We've imported our OpenAI agent, set up the API keys, and initialized our tool, checking the methods that it has available. Let's test out the tool before setting up our Agent.\n",
    "\n",
    "All of the Exa search tools make use of the `AutoPrompt` option where Exa will pass the query through an LLM to refine it in line with Exa query best-practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e64da618-b4ab-42d7-903d-f4eeb624f43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exa Tool] Autoprompt: Here is a comprehensive guide to machine learning transformers:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id_='348691a5-01af-4d94-9279-789038d8fa63', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Xavier Amatriain Mar 26, 2022 1 min read I no longer maintain my Medium blog. Find the newest (2023) version of the Tranformer catalog in my personal blog here.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6b42dfce-e05b-4c1d-83fe-138367f11aac', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='“The preeminent book for the preeminent transformers library—a model of clarity!”\\n—Jeremy Howard, cofounder of fast.ai and professor at University of Queensland\\n\\n“A wonderfully clear and incisive guide to modern NLP’s most essential library. Recommended!”\\n—Christopher Manning, Thomas M. Siebel Professor in Machine Learning, Stanford University\\n\\nBuy the book on Amazon\\nRead the book online at O’Reilly\\nDownload the book’s code \\nSince their introduction in 2017, transformers have quickly become the dominant architecture for achieving state-of-the-art results on a variety of natural language processing tasks. If you’re a data scientist or coder, this practical book shows you how to train and scale these large models using Hugging Face Transformers, a Python-based deep learning library.\\nTransformers have been used to write realistic news stories, improve Google Search queries, and even create chatbots that tell corny jokes. In this guide, authors Lewis Tunstall, Leandro von Werra, and Thomas Wolf use a hands-on approach to teach you how transformers work and how to integrate them in your applications. You’ll quickly learn a variety of tasks they can help you solve.\\n\\nBuild, debug, and optimize transformer models for core NLP tasks, such as text classification, named entity recognition, and question answering\\n Learn how transformers can be used for cross-lingual transfer learning\\n Apply transformers in real-world scenarios where labeled data is scarce\\n Make transformer models efficient for deployment using techniques such as distillation, pruning, and quantization\\n Train transformers from scratch and learn how to scale to multiple GPUs and distributed environments\\n\\nNews 🗞️\\nJanuary 31, 2022\\nLewis will be joining Abhishek Thakur to talk about the book and various techniques you can use to optimize Transformer models for production environments. We’ll also be giving away 5 copies of the book – join the event here!\\nJune 17, 2022\\nDue to the popularity of the book, O’Reilly has ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fc0c3986-f042-4950-b8e8-1a1e71a41a8a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='The famous paper “Attention is all you need” in 2017 changed the way we were thinking about attention. With enough data, matrix multiplications, linear layers, and layer normalization we can perform state-of-the-art-machine-translation. Nonetheless, 2020 was definitely the year of transformers! From natural language now they are into computer vision tasks. How did we go from attention to self-attention? Why does the transformer work so damn well? What are the critical components for its success? Read on and find out! In my opinion, transformers are not so hard to grasp. It\\'s the combination of all the surrounding concepts that may be confusing, including attention. That’s why we will slowly build around all the fundamental concepts. With Recurrent Neural Networks (RNN’s) we used to treat sequences sequentially to keep the order of the sentence in place. To satisfy that design, each RNN component (layer) needs the previous (hidden) output. As such, stacked LSTM computations were performed sequentially. Until transformers came out! The fundamental building block of a transformer is self-attention. To begin with, we need to get over sequential processing, recurrency, and LSTM’s! How? By simply changing the input representation! For a complete book to guide your learning on NLP, take a look at the \"Deep Learning for Natural Language Processing\" book. Use the code aisummer35 to get an exclusive 35% discount from your favorite AI blog :) Representing the input sentence Sets and tokenization The transformer revolution started with a simple question: Why don’t we feed the entire input sequence? No dependencies between hidden states! That might be cool! As an example the sentence “hello, I love you”:\\n\\nThis processing step is usually called tokenization and it\\'s the first out of three steps before we feed the input in the model. So instead of a sequence of elements, we now have a set. Sets are a collection of distinct elements, where the arrangement of the elements in the set', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exa_tool.search_and_retrieve_documents(\"machine learning transformers\", num_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c55a0c7b-4c58-4725-8543-29bb1b7278ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Transformers: a Primer',\n",
       "  'url': 'http://www.columbia.edu/~jsl2239/transformers.html',\n",
       "  'id': 'http://www.columbia.edu/~jsl2239/transformers.html'},\n",
       " {'title': 'Illustrated Guide to Transformers- Step by Step Explanation',\n",
       "  'url': 'https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0?gi=8fe76db5c4d9',\n",
       "  'id': 'https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0?gi=8fe76db5c4d9'},\n",
       " {'title': 'The Transformer Attention Mechanism - MachineLearningMastery.com',\n",
       "  'url': 'https://machinelearningmastery.com/the-transformer-attention-mechanism/',\n",
       "  'id': 'https://machinelearningmastery.com/the-transformer-attention-mechanism/'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exa_tool.find_similar(\n",
    "    \"https://www.mihaileric.com/posts/transformers-attention-in-disguise/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fc8665d-ddb8-411f-b187-93a132d19e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exa Tool] Autoprompt: Here is a summary of recent research around diffusion models:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id_='3e7c3b3d-e21b-4b7e-96b4-d5d8ccec12ab', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Getting-started-with-Diffusion-Literature\\nSummary of the most important papers and blogs about diffusion models for students to learn about diffusion models. Also contains an overview of all published robotics diffusion papers\\nLearning about Diffusion models\\nWhile there exist many tutorials for Diffusion models, below you can find an overview of some introduction blog posts and video, which I found the most intuitive and useful:\\n\\nWhat are Diffusion Models?: an introduction video, which introduces the general idea of diffusion models and some high-level math about how the model works\\n Generative Modeling by Estimating Gradients of the Data Distribution: blog post from the one of the most influential authors in this area, which introduces diffusion models from the score-based perspective\\n What are Diffusion Models: a in-depth blog post about the theory of diffusion models with a general summary on how diffusion model improved over time\\n Understanding Diffusion Models: an in-depth explanation paper, which explains the diffusion models from both perspectives with detailed derivations\\n\\nIf you don\\'t like reading blog posts and prefer the original papers, below you can find a list with the most important diffusion theory papers:\\n\\npaper link, Sohl-Dickstein, Jascha, et al. \"Deep unsupervised learning using nonequilibrium thermodynamics.\" International Conference on Machine Learning. PMLR, 2015.\\n paper link, Ho, Jonathan, Ajay Jain, and Pieter Abbeel. \"Denoising diffusion probabilistic models.\" Advances in Neural Information Processing Systems 33 (2020): 6840-6851.\\n paper link,Song, Yang, et al. \"Score-Based Generative Modeling through Stochastic Differential Equations.\" International Conference on Learning Representations. 2020.\\n paper link, Ho, Jonathan, and Tim Salimans. \"Classifier-Free Diffusion Guidance.\" NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications. 2021.\\n\\nOur current model implementation is based on this paper:\\n\\npaper link, Karras, Tero', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exa_tool.search_and_retrieve_documents(\n",
    "    \"This is a summary of recent research around diffusion models:\", num_results=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9325841-9f9a-4b9e-a602-fe542be8f364",
   "metadata": {},
   "source": [
    "While `search_and_retrieve_documents` returns raw text from the source document, `search_and_retrieve_highlights` returns relevant curated snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a64b37f6-ec12-45e8-9291-fa2fe51de311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exa Tool] Autoprompt: Here is a summary of recent research around diffusion models:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id_='c0fde261-f3b7-41b6-be5c-931ce5134242', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Diffusion models are a type of generative model and in this field, the main focus are vision based applications, thus all theory papers mentioned in the text below are mostly focused on image synthesis or similar tasks related to it. Diffusion models can be viewed from two perspectives: one is based on the initial idea of of Sohl-Dickstein et al., (2015) and the other is based on a different direction of research: score-based generative models. Song & Ermon, (2019) introduced the score-based generative models category. They presented the noise-conditioned score network (NCSN), which is a predecessor to diffusion model. The main idea of the paper is to learn the score function of the unknown data distribution with a neural network.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exa_tool.search_and_retrieve_highlights(\n",
    "    \"This is a summary of recent research around diffusion models:\", num_results=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2caa3a",
   "metadata": {},
   "source": [
    "### Exploring other Exa functionalities\n",
    "There are many additional parameters that you can pass to Exa methods to help find only what you need - namely domain and date filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "157d4aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exa Tool] Autoprompt: Here is a recent advancement in quantum computing:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id_='609ae891-42b3-4dcb-b6dd-4704ade56357', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Insider Brief\\n\\nQuantinuum and University of Colorado researchers successfully entangled four logical qubits with better fidelity than their physical counterparts.\\nThe advance demonstrates improved error protection and operational reliability, essential steps toward developing practical and scalable quantum computers.\\nThe achievement also shows Quantinuum’s commitment to making quantum computing more accessible and reliable, combining the advanced H2 quantum processor with innovative error-correcting codes.\\n\\nQuantinuum and the University of Colorado say they have — for the first time — entangled four error-protected logical qubits that have better fidelity than their physical counterparts. It’s not just an academic exercise, though. They report in a paper they posted on the pre-print server ArXiv that the system will improve the accuracy of quantum operations leading to an enhancement in error protection and operational reliability, both crucial steps towards making quantum computers a practical reality.\\nThe researchers added that they implemented the error correcting codes — called high-rate non-local quantum Low-Density Parity-Check (qLDPC) code — on Quantinuum’s H2 quantum processor.\\n The Challenge of Quantum Error Correction \\nQuantum error correction is a critical component in the quest for reliable and practical quantum computers, according to the team. Quantum systems are inherently fragile, susceptible to errors from environmental interference and imperfect operations. \\nThe researchers write in a blog post: “For a quantum computer to be useful, it must be universal, have lots of qubits, and be able to detect and correct errors. The error correction step must be done so well that in the final calculations, you only see an error in less than one in a billion (or maybe even one in a trillion) tries. Correcting errors on a quantum computer is quite tricky, and most current error correcting schemes are quite expensive for quantum computers to run.”\\nTo mitigate thes', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6257251c-68b7-482f-84b5-fe4370bbd1af', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aa9658df-7da5-411b-a015-20cd09e16c0c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='References Goldin, G. A., Menikoff, R. & Sharp, D. H. Comments on ‘general theory for quantum statistics in two dimensions’. Phys. Rev. Lett. 54, 603–603 (1985). Article \\n ADS \\n MathSciNet \\n CAS \\n PubMed\\n\\nGoogle Scholar \\n Moore, G. & Seiberg, N. Classical and quantum conformal field theory. Commun. Math. Phys. 123, 177–254 (1989). Article \\n ADS \\n MathSciNet\\n\\nGoogle Scholar \\n Moore, G. & Read, N. Nonabelions in the fractional quantum Hall effect. Nucl. Phys. B 360, 362–396 (1991). Article \\n ADS \\n MathSciNet\\n\\nGoogle Scholar \\n Wen, X. G. Non-Abelian statistics in the fractional quantum Hall states. Phys. Rev. Lett. 66, 802–805 (1991). Article \\n ADS \\n MathSciNet \\n CAS \\n PubMed\\n\\nGoogle Scholar \\n Kitaev, A. Y. Fault-tolerant quantum computation by anyons. Ann. Phys. 303, 2–30 (2003). Article \\n ADS \\n MathSciNet \\n CAS\\n\\nGoogle Scholar \\n Nayak, C., Simon, S. H., Stern, A., Freedman, M. & Das Sarma, S. Non-Abelian anyons and topological quantum computation. Rev. Mod. Phys. 80, 1083–1159 (2008). Article \\n ADS \\n MathSciNet \\n CAS\\n\\nGoogle Scholar \\n Wen, X.-G. Quantum Field Theory of Many-body Systems Oxford Graduate Texts (Oxford Univ. Press, 2010). Leinaas, J. M. & Myrheim, J. On the theory of identical particles. Nuovo Cim. B 37, 1–23 (1977). Article \\n ADS\\n\\nGoogle Scholar \\n Goldin, G. A., Menikoff, R. & Sharp, D. H. Representations of a local current algebra in nonsimply connected space and the Aharonov–Bohm effect. J. Math. Phys. 22, 1664–1668 (1981). Article \\n ADS \\n MathSciNet\\n\\nGoogle Scholar \\n Wilczek, F. Quantum mechanics of fractional-spin particles. Phys. Rev. Lett. 49, 957–959 (1982). Article \\n ADS \\n MathSciNet \\n CAS\\n\\nGoogle Scholar \\n Fowler, A. G., Mariantoni, M., Martinis, J. M. & Cleland, A. N. Surface codes: towards practical large-scale quantum computation. Phys. Rev. A 86, 032324 (2012). Article \\n ADS\\n\\nGoogle Scholar \\n Nakamura, J., Liang, S., Gardner, G. C. & Manfra, M. J. Direct observation of anyonic braiding statistics. Nat. Phys. 16, 931–936 (2020). Article \\n C', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1: Calling search_and_contents with date filters\n",
    "exa_tool.search_and_retrieve_documents(\n",
    "    \"Advancements in quantum computing\",\n",
    "    num_results=3,\n",
    "    start_published_date=\"2024-01-01\",\n",
    "    end_published_date=\"2024-07-10\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d6c30c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exa Tool] Autoprompt: Here is a link to a climate change mitigation strategy:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id_='c87bcad1-80eb-403e-825c-c2baed082500', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"November 14, 2006 103 (46) 17184-17189 Abstract If it were to become apparent that dangerous changes in global climate were inevitable, despite greenhouse gas controls, active methods to cool the Earth on an emergency basis might be desirable. The concept considered here is to block 1.8% of the solar flux with a space sunshade orbited near the inner Lagrange point (L1), in-line between the Earth and sun. Following the work of J. Early [Early, JT (1989) J Br Interplanet Soc 42:567–569], transparent material would be used to deflect the sunlight, rather than to absorb it, to minimize the shift in balance out from L1 caused by radiation pressure. Three advances aimed at practical implementation are presented. First is an optical design for a very thin refractive screen with low reflectivity, leading to a total sunshade mass of ≈20 million tons. Second is a concept aimed at reducing transportation cost to $50/kg by using electromagnetic acceleration to escape Earth's gravity, followed by ion propulsion. Third is an implementation of the sunshade as a cloud of many spacecraft, autonomously stabilized by modulating solar radiation pressure. These meter-sized “flyers” would be assembled completely before launch, avoiding any need for construction or unfolding in space. They would weigh a gram each, be launched in stacks of 800,000, and remain for a projected lifetime of 50 years within a 100,000-km-long cloud. The concept builds on existing technologies. It seems feasible that it could be developed and deployed in ≈25 years at a cost of a few trillion dollars, <0.5% of world gross domestic product (GDP) over that time.\\n\\nSign up for PNAS alerts.\\n Get alerts for new articles, or get an alert when an article is cited.\\n\\nProjections by the Intergovernmental Panel on Climate Change are for global temperature to rise between 1.5 and 4.5°C by 2100 (1), but recent studies suggest a larger range of uncertainty. Increases as high as 11°C might be possible given CO2 stabilizing at twi\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d79471de-0128-4d17-84f8-152f2bc6a316', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"() Abstract If it were to become apparent that dangerous changes in global climate were inevitable, despite greenhouse gas controls, active methods to cool the Earth on an emergency basis might be desirable. The concept considered here is to block 1.8% of the solar flux with a space sunshade orbited near the inner Lagrange point (L1), in-line between the Earth and sun. Following the work of J. Early [Early, JT (1989) J Br Interplanet Soc 42:567–569], transparent material would be used to deflect the sunlight, rather than to absorb it, to minimize the shift in balance out from L1 caused by radiation pressure. Three advances aimed at practical implementation are presented. First is an optical design for a very thin refractive screen with low reflectivity, leading to a total sunshade mass of ≈20 million tons. Second is a concept aimed at reducing transportation cost to $50/kg by using electromagnetic acceleration to escape Earth's gravity, followed by ion propulsion. Third is an implementation of the sunshade as a cloud of many spacecraft, autonomously stabilized by modulating solar radiation pressure. These meter-sized “flyers” would be assembled completely before launch, avoiding any need for construction or unfolding in space. They would weigh a gram each, be launched in stacks of 800,000, and remain for a projected lifetime of 50 years within a 100,000-km-long cloud. The concept builds on existing technologies. It seems feasible that it could be developed and deployed in ≈25 years at a cost of a few trillion dollars, <0.5% of world gross domestic product (GDP) over that time.\\n\\nSign up for PNAS alerts.\\nGet alerts for new articles, or get an alert when an article is cited.\\n\\nProjections by the Intergovernmental Panel on Climate Change are for global temperature to rise between 1.5 and 4.5°C by 2100 (1), but recent studies suggest a larger range of uncertainty. Increases as high as 11°C might be possible given CO2 stabilizing at twice preindustrial content (2). Holding\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cfa630bd-7988-4787-99c1-5300051677ba', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Main The notion of offsetting global warming by means of solar climate intervention, or solar radiation modification (SRM), is at the centre of an intense debate on the intrinsic governance dilemma 1,2,3 . Stratospheric aerosol deposition, the injection of particles reflecting incoming solar radiation into the stratosphere as a means to limit global warming, is one form of SRM. Substantial SRM interventions promise, in theory, to stop global warming quasi-instantaneously 4,5 and at relatively limited cost 6,7 . The door to limit climate warming to well below 2 °C via greenhouse gas mitigation within the twenty-first century is rapidly closing 8,9 . As every decade of delayed emissions reduction increases the committed warming 10 , SRM interventions might be seriously considered in the future. While the effectiveness of SRM to halt anthropogenic climate change has been quantified by modelling studies 11,12,13 , the regional impacts of planetary SRM remain highly uncertain 14 . This is due to an insufficient understanding of the responses of the hydrological cycle 14 and atmospheric chemistry 4 to such interventions. Potential side effects of SRM range from shifting monsoon patterns 15 to changes in the meridional temperature gradients 16 , in atmospheric and oceanic circulations 17,18 and in the statistics of the modes of climate variability 18 , all of which are insufficiently studied 3 . In summary, SRM could have impacts on regional weather patterns detrimental to society and the biosphere 19,20,21 , and other still-unknown effects, while not addressing the direct adverse effects of rising atmospheric CO2 such as ocean acidification 22 . Furthermore, the effect of SRM on the polar ice sheets remains largely unexplored. SRM research to date has been confined to model simulations and laboratory studies. Field experiments that mimic natural analogues such as volcanic eruptions (for example, the Stratospheric Controlled Perturbation Experiment 23 ) have encountered so', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2: Calling search_and_contents with include_domains filters\n",
    "exa_tool.search_and_retrieve_documents(\n",
    "    \"Climate change mitigation strategies\",\n",
    "    num_results=3,\n",
    "    include_domains=[\"www.nature.com\", \"www.sciencemag.org\", \"www.pnas.org\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1210906d-87a7-466a-9712-1d17dba2c2ec",
   "metadata": {},
   "source": [
    "We can see we have different tools to search for results, retrieve the results, find similar results to a web page, and finally a tool that combines search and document retrieval into a single tool. We will test them out in LLM Agents below:\n",
    "\n",
    "### Using the Search and Retrieve documents tools in an Agent\n",
    "\n",
    "We can create an agent with access to the above tools and start testing it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d88c2ee-184a-4371-995b-a086b34db24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't give the Agent our unwrapped retrieve document tools, instead pass the wrapped tools\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    exa_tool_list,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f69a53fd-55c4-4e18-8fbe-6a29d5f3cef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What are the best resturants in toronto?\n",
      "=== Calling Function ===\n",
      "Calling function: search with args: {\"query\":\"best restaurants in Toronto\",\"num_results\":5}\n",
      "[Exa Tool] Autoprompt: One of the best restaurants in Toronto is:\n",
      "Got output: [{'title': 'Via Allegro Ristorante - Toronto Fine Dining Restaurant', 'url': 'https://viaallegroristorante.com/', 'id': 'https://viaallegroristorante.com/'}, {'title': 'Sophisticated Dining - Toronto Restaurant | Scaramouche', 'url': 'https://www.scaramoucherestaurant.com/', 'id': 'https://www.scaramoucherestaurant.com/'}, {'title': \"Where To Eat In Toronto - Barberian's Steak House\", 'url': 'https://www.elitesportstours.ca/blog/where-to-eat-in-toronto-barberians-steakhouse', 'id': 'https://www.elitesportstours.ca/blog/where-to-eat-in-toronto-barberians-steakhouse'}, {'title': 'Where To Eat In Toronto - STK Toronto', 'url': 'https://www.elitesportstours.ca/blog/where-to-eat-in-toronto-stk-toronto', 'id': 'https://www.elitesportstours.ca/blog/where-to-eat-in-toronto-stk-toronto'}, {'title': 'Michelin Star Fine Dining Restaurant', 'url': 'https://www.donalfonsotoronto.com/', 'id': 'https://www.donalfonsotoronto.com/'}]\n",
      "========================\n",
      "\n",
      "Here are some of the best restaurants in Toronto:\n",
      "\n",
      "1. [Via Allegro Ristorante](https://viaallegroristorante.com/)\n",
      "2. [Scaramouche](https://www.scaramoucherestaurant.com/)\n",
      "3. [Barberian's Steak House](https://www.elitesportstours.ca/blog/where-to-eat-in-toronto-barberians-steakhouse)\n",
      "4. [STK Toronto](https://www.elitesportstours.ca/blog/where-to-eat-in-toronto-stk-toronto)\n",
      "5. [Don Alfonso 1890 Toronto](https://www.donalfonsotoronto.com/)\n",
      "\n",
      "You can explore these restaurants for a great dining experience in Toronto!\n"
     ]
    }
   ],
   "source": [
    "print(agent.chat(\"What are the best resturants in toronto?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f44035e9-27ab-47b7-abc5-cf2fe5d1482f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: tell me more about Osteria Giulia\n",
      "=== Calling Function ===\n",
      "Calling function: search with args: {\"query\":\"Osteria Giulia restaurant Toronto\",\"num_results\":1}\n",
      "[Exa Tool] Autoprompt: A fantastic restaurant to try in Toronto is Osteria Giulia.\n",
      "Got output: [{'title': 'Giulietta', 'url': 'https://giu.ca/', 'id': 'https://giu.ca/'}]\n",
      "========================\n",
      "\n",
      "It seems that there might be a slight confusion in the search results. The search returned information about a restaurant named \"Giulietta\" instead of \"Osteria Giulia.\" Would you like me to search again for more specific information about Osteria Giulia in Toronto?\n"
     ]
    }
   ],
   "source": [
    "print(agent.chat(\"tell me more about Osteria Giulia\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939c7b98-0d75-4ef0-ac47-fd3bd24d3e50",
   "metadata": {},
   "source": [
    "## Avoiding Context Window Issues\n",
    "\n",
    "The above example shows the core uses of the Exa tool. We can easily retrieve a clean list of links related to a query, and then we can fetch the content of the article as a cleaned up html extract. Alternatively, the search_and_retrieve_documents tool directly returns the documents from our search result.\n",
    "\n",
    "We can see that the content of the articles is somewhat long and may overflow current LLM context windows.  \n",
    "\n",
    "1. Use `search_and_retrieve_highlights`: This is an endpoint offered by Exa that directly retrieves relevant highlight snippets from the web, instead of full web articles. As a result you don't need to worry about indexing/chunking offline yourself!\n",
    "\n",
    "2. Wrap `search_and_retrieve_documents` with `LoadAndSearchToolSpec`: We set up and use a \"wrapper\" tool from LlamaIndex that allows us to load text from any tool into a VectorStore, and query it for retrieval. This is where the `search_and_retrieve_documents` tool become particularly useful. The Agent can make a single query to retrieve a large number of documents, using a very small number of tokens, and then make queries to retrieve specific information from the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8c51fc-8a22-408e-94c9-14248bad61c1",
   "metadata": {},
   "source": [
    "### 1. Using `search_and_retrieve_highlights`\n",
    "\n",
    "The easiest is to just use `search_and_retrieve_highlights` from Exa. This is essentially a \"web RAG\" endpoint - they handle chunking/embedding under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "197d241b-cd53-4038-a824-d493c69166b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = exa_tool.to_tool_list(\n",
    "    spec_functions=[\"search_and_retrieve_highlights\", \"current_date\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c412501-4fa3-4bb5-a324-075e809737d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = OpenAIAgent.from_tools(\n",
    "    tools,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "242c779e-9dbc-4aec-8838-c152bf8f304b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me more about the recent news on semiconductors\n",
      "=== Calling Function ===\n",
      "Calling function: current_date with args: {}\n",
      "Got output: 2024-07-10\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: search_and_retrieve_highlights with args: {\"query\":\"semiconductors\",\"num_results\":5,\"start_published_date\":\"2024-06-01\"}\n",
      "[Exa Tool] Autoprompt: A cutting-edge company in the field of semiconductors is:\n",
      "Got output: [Document(id_='f0cb0faf-7506-4c44-b952-031ae92953ae', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Silicon carbide is a critical material for high-power, high-temperature applications, and is extremely difficult to produce. onsemi is one of the only companies in the world with the ability to manufacture SiC-based semiconductors from crystal growth to advanced packaging solutions. onsemi’s plan to expand SiC manufacturing with a multi-year brownfield investment of up to $2 billion (44 billion CZK) is part of the company’s previously disclosed long-term capital expenditure target. This investment would build on the company’s current operations in the Czech Republic, which include silicon crystal growth, silicon and silicon carbide wafer manufacturing (polished and EPI) and a silicon wafer fab. Today, the site can produce more than three million wafers annually, including more than one billion power devices.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='4128552e-83b5-4698-8979-1c8c090ad53e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='By 2025, wafer fab equipment sales are projected to increase by 14.7%, reaching $113 billion, propelled by advanced logic and memory applications. After two years of contraction, the back-end equipment segment is expected to recover in the latter half of 2024. Sales of semiconductor test equipment are anticipated to rise by 7.4% to $6.7 billion, while assembly and packaging equipment sales are predicted to grow by 10.0% to $4.4 billion. This growth is expected to accelerate in 2025, with test equipment sales surging by 30.3% and assembly and packaging sales increasing by 34.9%. This rebound is supported by the growing complexity of semiconductor devices for high-performance computing and a recovery in demand across automotive, industrial, and consumer electronics markets.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d2deca54-528e-465a-9a61-6348cc01ad42', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='A Silicon Box signage is pictured during the opening of their advanced semiconductor manufacturing foundry in Singapore July 20, 2023. REUTERS/Edgar Su/File Photo Silicon Box, a Singapore-based semiconductor design and device integration services startup, reported a 3.2x surge in losses for the financial year ended December 31, 2023, even as it declared revenue for the first time since its inception in 2021, regulatory filings showed. Start your deal-making journey now! Subscribe now to enjoy unlimited access at just $59. Premium coverage on private equity, venture capital, and startups in Asia.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='2e64e8bc-72e9-4064-8267-a808e3def733', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Production and Investment: Production is expected to start in 2026, with full capacity reached by 2033, producing up to 15,000 wafers weekly. The total investment is projected at about five billion euros, with around two billion euros in support from the Italian government under the EU Chips Act. Sustainability Commitment: The campus design and operations will emphasize sustainable practices, particularly in resource consumption like water and power. “The fully integrated capabilities unlocked by the Silicon Carbide Campus in Catania will contribute significantly to ST’s SiC technology leadership for automotive and industrial customers through the next decades,” said Jean-Marc Chery, President and Chief Executive Officer of STMicroelectronics. “The scale and synergies offered by this project will enable us to better innovate with high-volume manufacturing capacity, to the benefit of our European and global customers as they transition to electrification and seek more energy efficient solutions to meet their decarbonization goals.”', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='2762ccd9-eb83-4aa2-8ead-cab67f5267d9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"Vanguard, which made a $236 million acquisition of a less advanced wafer facility in Singapore from New York-based contract chipmaker GlobalFoundries in 2019, said the new plant will help it diversify its manufacturing operations. Singapore has attracted investments from several semiconductor companies, aided by its business-friendly environment. GlobalFoundries opened a $4 billion chip fabrication plant in Singapore last year, with its president lauding the government's industrial policies. In 2022 Taiwan's United Microelectronics Corp invested $5 billion into its Singapore microchip factory. Neighbour Malaysia has also emerged as a hotspot for semiconductor companies, with investments from American chip giants Intel and GlobalFoundries.\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n",
      "========================\n",
      "\n",
      "Response: Here are some recent highlights on semiconductors:\n",
      "\n",
      "1. **Expansion of SiC Manufacturing**: onsemi, one of the few companies with the ability to manufacture SiC-based semiconductors, plans to expand its SiC manufacturing with a multi-year brownfield investment of up to $2 billion. This investment will build on the company's current operations in the Czech Republic, including silicon crystal growth, silicon and silicon carbide wafer manufacturing, and a silicon wafer fab.\n",
      "\n",
      "2. **Projected Growth in Wafer Fab Equipment Sales**: By 2025, wafer fab equipment sales are projected to increase by 14.7%, reaching $113 billion. This growth is driven by advanced logic and memory applications. The back-end equipment segment is expected to recover in the latter half of 2024, with semiconductor test equipment sales anticipated to rise by 7.4% and assembly and packaging equipment sales predicted to grow by 10.0%.\n",
      "\n",
      "3. **Silicon Box's Financial Performance**: Singapore-based semiconductor design and device integration services startup, Silicon Box, reported a 3.2x surge in losses for the financial year ended December 31, 2023. Despite this, the company declared revenue for the first time since its inception in 2021.\n",
      "\n",
      "4. **Investment in Silicon Carbide Campus**: STMicroelectronics is set to start production in 2026 at its Silicon Carbide Campus in Catania, with full capacity reached by 2033. The total investment is projected at about five billion euros, with support from the Italian government under the EU Chips Act. The campus design and operations will emphasize sustainable practices.\n",
      "\n",
      "5. **Vanguard's Wafer Facility Acquisition**: Vanguard acquired a less advanced wafer facility in Singapore from GlobalFoundries in 2019. This acquisition aims to diversify Vanguard's manufacturing operations. Singapore has attracted investments from various semiconductor companies due to its business-friendly environment.\n",
      "\n",
      "These are some of the recent developments in the semiconductor industry.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Tell me more about the recent news on semiconductors\")\n",
    "print(f\"Response: {str(response)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c801b9-7f61-470b-9d05-c00622d5fbd7",
   "metadata": {},
   "source": [
    "### 2. Using `LoadAndSearchToolSpec`\n",
    "\n",
    "Here we wrap the `search_and_retrieve_documents` functionality with the `load_and_search_tool_spec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a017cc61-1696-4a03-8d09-a628f9049cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools.tool_spec.load_and_search import (\n",
    "    LoadAndSearchToolSpec,\n",
    ")\n",
    "\n",
    "# The search_and_retrieve_documents tool is the third in the tool list, as seen above\n",
    "search_and_retrieve_docs_tool = exa_tool.to_tool_list(\n",
    "    spec_functions=[\"search_and_retrieve_documents\"]\n",
    ")[0]\n",
    "date_tool = exa_tool.to_tool_list(spec_functions=[\"current_date\"])[0]\n",
    "wrapped_retrieve = LoadAndSearchToolSpec.from_defaults(search_and_retrieve_docs_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b47437-8f6d-4e94-97ca-4e35f78336f2",
   "metadata": {},
   "source": [
    "Our wrapped retrieval tools separate loading and reading into separate interfaces. We use `load` to load the documents into the vector store, and `read` to query the vector store. Let's try it out again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4f81bd3-a5b9-452c-93f4-91d16c4c0df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exa Tool] Autoprompt: Here is the best explanation for machine learning transformers:\n",
      "A transformer is a type of neural network architecture that is well-suited for tasks involving processing sequences as inputs. It is designed to create a numerical representation for each element within a sequence, capturing essential information about the element and its surrounding context. Transformers have been instrumental in revolutionizing natural language processing tasks, such as translation and autocomplete services, by leveraging their ability to understand and generate natural language text.\n",
      "The first paper on transformers was written in 2017.\n"
     ]
    }
   ],
   "source": [
    "wrapped_retrieve.load(\"This is the best explanation for machine learning transformers:\")\n",
    "print(wrapped_retrieve.read(\"what is a transformer\"))\n",
    "print(wrapped_retrieve.read(\"who wrote the first paper on transformers\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85be6977-c4e8-43a4-99be-3322d4b72b07",
   "metadata": {},
   "source": [
    "## Creating the Agent\n",
    "\n",
    "We now are ready to create an Agent that can use Exa's services to their full potential. We will use our wrapped read and load tools, as well as the `get_date` utility for the following agent and test it out below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a893f26-dbb6-4b72-9795-702eaf749564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just pass the wrapped tools and the get_date utility\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    [*wrapped_retrieve.to_tool_list(), date_tool],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5835d058-da9c-4d42-9d2a-941c73b88a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    agent.chat(\n",
    "        \"Can you summarize everything published in the last month regarding news on superconductors\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ee91ca-6730-4fdd-8189-ac21022f34f1",
   "metadata": {},
   "source": [
    "We asked the agent to retrieve documents related to superconductors from this month. It used the `get_date` tool to determine the current month, and then applied the filters in Exa based on publication date when calling `search`. It then loaded the documents using `retrieve_documents` and read them using `read_retrieve_documents`.\n",
    "\n",
    "We can make another query to the vector store to read from it again, now that the articles are loaded:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
